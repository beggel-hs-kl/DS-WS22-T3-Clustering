---
title: "Centroid Clustering (K-means) - Credit Card dataset"
output: html_document
---

# DS: K-means Clustering with Credit Card Data

## Introduction


In this notebook we are considering credit cards customers segmentation with the help of K-means.

K-means is a popular and simple unsupervised learning technique that is used to group similar data points together. The goal of this notebook is to use this algorithm to identify patterns and relationships in the credit card spending habits of the customers. This can then be used to inform targeted marketing campaigns or other business strategies.

We will begin by loading and exploring the credit card dataset, which contains information such as the customer's monthly spending and credit limit. We will then pre-process the data to prepare it for clustering. Next, we will use the k-means algorithm to cluster the customers into groups based on their spending habits. Finally, we will analyze the results of the clustering and discuss how the information can be used to inform business decisions.

Throughout the notebook, we will be using the R programming language and various R libraries such as ggplot2 for visualization, dplyr for data manipulation and kmeans for cluster analysis.


## K-means Clustering

K-means is an unsupervised machine learning algorithm that is used for clustering. It divides a group of data points into a specified number (k) of clusters, where each data point is assigned to the cluster with the closest mean. The goal is to find clusters such that the data points within each cluster are as similar as possible to each other, and as dissimilar as possible to data points in other clusters.

Here's a high-level summary of the k-means algorithm:

1. Initialize k centroids (randomly or based on some heuristic).
2. Assign each data point to the nearest centroid, forming k clusters.
3. Calculate the mean of each cluster.
4. Reassign each data point to the nearest centroid using the updated means.
5. Repeat steps 3 and 4 until the assignments do not change or some other stopping criteria is met.


K-means is a widely used and relatively simple algorithm. It is easy to understand and runs in linear time. Above that, it can deal well with large datasets. But it has some limitations. For example, it is sensitive to the initial choice of centroids, and it can get stuck in local minima. It also assumes that the clusters are spherical, which may not always be the case in real-world data. Other than that, the number of clusters must be chosen manually. Luckily, there are some algorithms to define an optimal number of clusters. K-means is also not very robust to outliers (either centroids might get dragged by them or they could be their own clusters instead of being ignored) and large number of dimensions (as the number of dimensions increases, a distance-based similarity measure converges to a constant value between any given examples).

## Dataset Description

The dataset for retrieved from:
[Kaggle: Credit Card Dataset for Clustering](https://www.kaggle.com/datasets/arjunbhasin2013/ccdata)*

It summarizes the usage behaviour of 8950 active credit card holders during the last 6 months and contains the following 18 variables:

* **CUST_ID** : Identification of Credit Card holder (Categorical)
* **BALANCE** : Total amount of money a customer owes to the bank. Monthly average balance, based on daily balance average. Increases when a purchase is made, decreases when a payment is made.
* **BALANCE_FREQUENCY** : How frequently the Balance is updated, score between 0 and 1 (1 = frequently updated, 0 = not frequently updated).
* **PURCHASES** : Total amount spend on purchases.
* **ONEOFF_PURCHASES** : Maximum purchase amount done in one-go.
* **INSTALLMENTS_PURCHASES** : Overall amount of purchases done in installment.
* **CASH_ADVANCE** : Cash in advance given by the user (Paying before the goods are shipped).
* **PURCHASES_FREQUENCY** : How frequently the Purchases are being made, score between 0 and 1 (1 = frequently purchased, 0 = not frequently purchased).
* **ONEOFFPURCHASESFREQUENCY** : How frequently Purchases are happening in one-go (1 = frequently purchased, 0 = not frequently purchased).
* **PURCHASESINSTALLMENTSFREQUENCY** : How frequently purchases in installments are being done (1 = frequently done, 0 = not frequently done).
* **CASHADVANCEFREQUENCY** : How frequently the cash in advance is being paid.
* **CASHADVANCETRX** : Number of Transactions made with "Cash in Advanced".
* **PURCHASES_TRX** : Number of purchase transactions made.
* **CREDIT_LIMIT** : Limit of Credit Card for user.
* **PAYMENTS** : Amount of Payment done by user to decrease their statement balance.
* **MINIMUM_PAYMENTS** : Total minimum payments due in the period. Minimum payments are paid to avoid interest on purchases.
* **PRCFULLPAYMENT** : Percent of full payment paid by user.
* **TENURE** : Tenure of credit card service for user (Since how many months a customer uses a credit card).


## Load Libraries

The following libraries will be needed to run this notebook:

### dplyr

dplyr provides a set of commands for manipulating data frames. It is designed to make it easy to perform data manipulation tasks in R in a consistent, efficient, and expressive way. The package provides a set of functions for filtering, selecting, and transforming data, as well as for performing aggregate calculations and data summaries.

### factoextra

factoextra provides additional functionality for exploratory factor analysis (EFA) and principal component analysis (PCA). It is designed to be used in conjunction with other packages (such as ggplot2) to enhance the visualization and interpretation of EFA and PCA results. Although it can also be used for determining and visualizing a number of clusters.

### GGally

GGally is an R package that provides a set of functions for creating enhanced scatter plots and correlation matrices using the ggplot2 package. It provides a convenient way to plot complex data sets, and to visualize the relationship between different variables in the data. GGally is an easy to use package which can help users to explore and understand the structure of their data.

### ggplot2

ggplot2 is a popular package that provides an easy and powerful way to create static and interactive plots and graphics. It allows to easily create and customize a wide variety of plots. It provides a consistent and expressive syntax for creating different types of plots, including scatter plots, line plots, bar plots, histograms, density plots, and more. It is widely adopted by data analysts, statisticians and data scientist due to its flexibility, expressiveness, and its capability to handle complex data sets.

### gridExtra

gridExtra provides additional functionality for creating complex and custom graphics using the grid graphics system, which is the underlying system used by ggplot2. The grid system provides a powerful way to create custom layouts and arrange multiple plots and other graphical elements on a page, and gridExtra extends this functionality by providing a set of convenient functions for creating common types of multi-panel plots and tables. gridExtra is not as widely used as ggplot2 but it's very useful when you want to create complex graphics, when you need to combine multiple plots and other graphical elements, or when you need to create custom multi-panel plots. It's an essential tool when you need to fine-tune the representation of your visualizations in a publication-ready format.

### imputeTS

imputeTS is an R package for time series imputation. Time series imputation is the process of filling in missing data points in a time series. This can be useful when working with real-world data, which often contains missing values due to various reasons such as sensor failure or data collection errors. For example, using this package, one can replace missing data in the dataset with the mean or median values.

### tidyverse

The tidyverse is a collection of R packages that are designed to work together to make data manipulation and visualization easy and consistent. It's one of the most widely used collection of packages for data manipulation, visualization, and modeling in R. It includes a set of widely used and well-documented packages, including ggplot2 for data visualization, dplyr for data manipulation, tidyr for data reshaping, readr for reading in data, purrr for functional programming and more. As ggplot2 and dplyr are already included in this package, we do not have to load them separately.

The packages in the tidyverse are designed to work together, making it easy to chain together multiple operations to accomplish a task. Each package in the tidyverse is focused on solving a specific problem, while providing a consistent and easy to use interface. The collection of packages in the tidyverse is actively developed and maintained, and it's widely used in academia and industry, by data scientists, statisticians, and researchers alike. The package is open-source, well-documented and actively maintained, making it a powerful and popular tool for data manipulation and visualization in R.


Loading the required packages:

```{r, warning=FALSE}
library(cluster)
library(factoextra)
library(GGally)
library(gridExtra)
library(imputeTS)
library(tidyverse)
```

## Load Data

In R, you can use the read.csv() function to read a CSV (Comma Separated Values) file and store it as a data frame.

Another way to do so is to use the read_csv() from readr package (included in tidyverse). It's a more efficient alternative to read csv files and handle large datasets. It returns a tibble (an enhanced dataframe) with all columns treated as their true types and better handle for reading large datasets.

We will use a relative path to get our data:


```{r}
cards_raw <- read_csv('./datasets/CC_GENERAL.csv')
head(cards_raw)
```

Here we see, how the data looks like.

Next, we will use str() function to display the internal structure of a dataset. It provides a quick and easy way to get a summary of the data types, dimensions, and some other attributes of a dataset:

```{r}
str(cards_raw)
```


As we can see, all the values other than CUST_ID are numeric ones. We will use the numeric attributes to perform K-means clustering.

Then we apply the summary() function to the dataset, in order to generate a summary statistics of it. It provides a quick and easy way to get an overview of the central tendency, dispersion, and other descriptive statistics of a dataset. It will display:

* the mean, median and quartiles for numerical variables
* the range for variables

```{r}
summary(cards_raw)
```

## Data Preparation

What we can see from the summary is that, for example, that people owe on average about 1.5K$ on their credit card. However, it is nearly 3 times smaller than the average credit limit. One can also see, that people make installment purchases with their credit card more often, than one-off purchases. But unexpectedly, they pay more for one-off purchases, than they do for the installment ones. Furthermore, the dataset primarily contains the information about long-term customers: the average tenure is more than 11 months.

What is also important to notice: there are missing values in the dataset. The affected columns are: CREDIT_LIMIT (1 NA values) and MINIMUM_PAYMENTS (313 NA values). K-means cannot deal with empty values, that's why we are to either replace or eliminate the affected rows.

There are multiple ways to deal with the missing values for K-means algorithm:

* **Remove missing values**: The simplest way to deal with missing values is to remove any observations that have missing values. This approach is straightforward, but it can also result in loss of important information. Additionally, if the missing data is not missing completely at random this can cause bias in the clustering.

* **Impute missing values**: Another approach is to impute missing values using a method such as mean or median imputation. Imputing the missing values allows you to keep the observations in the dataset and can improve the performance of the algorithm. However, imputing values might add noise and may not be the best strategy if the proportion of missing values is large.

* **Clustering with missing values**: Instead of removing or imputing missing values, there are also clustering algorithms (modification of a classic k-means) that can handle missing values. Some of them are k-means with missing values, k-modes and k-prototypes, which can handle categorical data and missing values simultaneously.

* **Using Expectation-Maximization (EM)**: Another approach is to use Expectation-Maximization algorithm which can handle missing data by iteratively imputing missing data estimates and estimating the parameters of the clustering model until the algorithm reaches convergence.

We'll choose to impute the missing values, so that we do not lose information. It should not add much noise, as the percentage of rows containing NaN values is low. Mean imputation is normally used for approximately normal distributed data. In this case, preserving the mean of the variable is important. On the other hand, when the data is heavily skewed or if the goal of the analysis is to preserve the median of the variable, then using the median to impute missing values may be appropriate. Median imputation is more robust to outliers and skewness than mean imputation.

We will check the data for outliers using the box plot:

```{r}
cards_raw %>%
        gather(Attributes, values, 2:18) %>%
        ggplot(aes(x = reorder(Attributes, values, FUN = median), y = values, fill = Attributes)) +
        geom_boxplot(show.legend = FALSE) +
        labs(title = "Cards Attributes - Boxplots") +
        theme_bw() +
        theme(axis.title.y = element_blank(),
              axis.title.x = element_blank()) +
        coord_flip()
```

...

As we are dealing with continuous data with many outliers, the median values will therefore be used to deal with the missing values. na_mean function from the imputeTS package helps us replace the missing values. With the help of complete.cases() function, we check if there are no NA values in the dataset anymore:

```{r}
cards <- na_mean(cards_raw, option = "median")
sum(!complete.cases(cards))
```

The next step of data preparation is scaling. Scaling is often used in K-means clustering to ensure that the variables are on the same scale before the algorithm is applied. The reason for this is that K-means uses the Euclidean distance metric to calculate the distance between observations and cluster centers, which can be affected by the scale of the variables.

If the variables have different scales, then the distance between observations and cluster centers will be dominated by the variable with the largest scale, giving it more weight in the clustering process. This can lead to suboptimal clusters and can cause the algorithm to converge to a local optimum.

Scaling the variables before applying K-means will ensure that the distances between observations and cluster centers are based on all variables, not just the variable with the largest scale. It helps to equalize the importance of all variables, so that the algorithm converges to the global optimum.

By scaling, the mean is subtracted from each variable, which then gets divided by the standard deviation. As a result, all the variable will have e mean of 0 and a standard deviation of 1.

Among all the variables, only CUST_ID does not give any benefit for the behavioral clustering. We are considering all other, numerical, attributes for the scaling. For scaling, we will use the scale() function:

```{r}
cards_norm <- as.data.frame(scale(cards[2:18], center = TRUE, scale = TRUE))
head(cards_norm)
```


Now we can see, that all the attributes are on the same scale.

## Dimension Reduction

K-means algorithm is sensitive to the number of variables. As the number of variables increases, it can lead to over-fitting, slow convergence, and suboptimal clusters. To reduce the dimensionality of the data, Principal Component Analysis (PCA) can be used.

PCA reduces the dimensionality of a dataset by projecting it onto a new set of axes that account for the most variation in the data. These new axes, called principal components, are linear combinations of the original variables that are ordered by the amount of variance they explain. By using only the first few principal components, you can retain most of the variation in the data while reducing the number of variables.

When used in conjunction with K-means, PCA can significantly speed up the clustering process, while also improving the quality of the clusters. It allows K-means to focus on the most important features of the data, reducing the influence of noise and irrelevant variables.

It's worth noting that PCA is most useful when applied to large datasets with many variables.

So, the next step we are applying PCA to our normalized dataset. For this purpose we will use the prcomp() function:

```{r}
pca_cards <- prcomp(cards_norm, center = TRUE)
summary(pca_cards)
```

The explanation of the output:

* **Standard deviation** - This is the standard deviation of the principal components. It represents the amount of variation in the data that is captured by each component. The first component will have the highest standard deviation, and the subsequent components will have decreasing standard deviations.
* **Proportion of Variance** - The proportion of variance explained by each principal component. This can be used to determine the relative importance of each component in explaining the variation in the data. As we can see, the last component (PC17) is not relevant at all.
* **Cumulative Proportion** - The proportion of the total variance in the data that is explained by each principal component. This can be used to determine the number of principal components to retain in the analysis.

Usually, when determining, how many components to keep, the goal is to reach 85-95% of the cumulative proportion. Another approach is to plot the Proportion of Variance for each of the components. The point where the plot starts to level off will show the desirable number of components to retain. The choice might also be influenced by the computational power we have. If we are dealing with a large dataset and do ot have much computational power, it makes sense to choose a smaller number of components. But in the end, it makes sense to try out several amounts of components in order to see, which of them leads to the best clustering.

We choose the first two Principal Components, since taking more of them leads to less accurate clustering of the data given. These will be our 2 dimensions for clustering. Having only 2 dimensions helps visualise the clusters on a 2-D scatter plot, reduce needed to perform clustering resources and avoid overfitting, that can occur when using a lot of dimensions.


```{r}
pca_cards <- as.data.frame(-pca_cards$x[, 1:2])
```

## Clustering

We will define a function for choosing the optimal K-value. There are several ways to do so. Usually the optimal number of clusters for the K-means algorithm is defined by using either elbow ot silhouette method.

The **elbow method** looks at the within-cluster variance (or within-cluster sum of squares, WCSS), also known as the **inertia**, and plots it as a function of the number of clusters. The idea behind the elbow method is that as the number of clusters increases, the WCSS decreases; however, at some point, the decrease in WCSS begins to slow down, forming an "elbow" shape when plotted on a graph. The idea is to choose a number of clusters where the decrease in variance begins to level off. This point is considered the optimal number of clusters.

The **silhouette method**, on the other hand, looks at the similarity of each point to the points in its own cluster compared to the next closest cluster. It then plots the silhouette coefficient for each point, where a coefficient close to 1 indicates that the point is well-matched to its own cluster and clusters are easily distinguishable, and a coefficient close to -1 indicates that the point is mis-matched to its own cluster. A coefficient equal to 0 indicates the overlapping of the clusters. The optimal number of clusters is chosen as the number of clusters that result in the highest overall silhouette coefficient.

Both methods are useful for determining the optimal number of clusters, however the silhouette method is considered more robust and is less sensitive to the initial conditions of the clustering algorithm. Also, it is not always clear, where the Elbow Point on the graph is, when using the elbow method. When working with the real-world datasets, we often experience, that there are several points, where the variance changes its behaviour.

We will try both methods out.

For the visualisation, the fviz_nbclust() will be used. The grid.arrange() function helps us create a horizontal layout, consisting of 2 plots:

```{r}
find_optimal_k <- function(dataset) {
  wss_res <- fviz_nbclust(dataset, kmeans, method = "wss")
  sil_res <- fviz_nbclust(dataset, kmeans, method = "silhouette")
  grid.arrange(wss_res, sil_res, ncol = 2)
}
```

Now, we will analyze, what the optimal K-value could be:


```{r}
find_optimal_k(pca_cards)
```

Looking at the plot, constructed using the elbow method, one could say, the K-value could be 3. Although the WCSS value keeps changing relatively rapidly till the value 4. To get rid of confusion, we will take a look on the second plot. It clearly defines 3 to be the perfect value. It will then be our number of clusters.

Now, we are ready to apply the K-means algorithm. The function kmeans() is used in R for this purpose, where centers is the number of clusters:

```{r}
set.seed(1234)

pca_cards_clust <- kmeans(pca_cards, centers = 3)
table(pca_cards_clust$cluster)
```

We can see, that much more credit card holder belong to the 1st cluster, than to the other two. Next, it would be nice to visualize the clusters. So, we define a function plot_clusters(). We can also specify the colors, our cluster will be. We will define them in the clusters_palette variable:

```{r}
clusters_palette <- c("coral1", "yellow4", "mediumseagreen", "deepskyblue", "darkorchid1")
plot_clusters <- function(clusters_info, dataset) {
  fviz_cluster(clusters_info, data = dataset,
               geom = "point",
               palette = clusters_palette,
               ellipse.type = "convex",
               ggtheme = theme_bw()
  )
}
```
Let's apply this function to see what our clusters look like:

```{r}
plot_clusters(pca_cards_clust, pca_cards)
```

In a perfect case, the clusters would have certain distance from one another. But when we are dealing with the real-world data, it can happen, that the clusters are placed close to each other, as on this plot.

We will evaluate the clustering with the help of the silhouette function.

```{r}
sil <- silhouette(pca_cards_clust$cluster, dist(pca_cards))
fviz_silhouette(sil,
                palette = clusters_palette,)
```

An average silhouette width is 0.46. The closer this value to 1, the higher the similarity of each point to its own cluster than to other clusters.

After that, we will add the cluster flag to the original dataset:

```{r}
cards$cluster <- pca_cards_clust$cluster
```


## Cluster Analysis

We will create a pair of new attributes, that will help us understand the clusters:

```{r}
cards$balance.to.limit.ratio <- cards$BALANCE / cards$CREDIT_LIMIT
cards$payment.to.minimum.ratio <- cards$PAYMENTS / cards$MINIMUM_PAYMENTS
```
* Balance-to-limit ratio will show us, how close a customer gets on his credit limit.
* Payment-to-minimum - whether a customer pays a minimum payment or probably gets interest on what he owes to the bank.

We will not analyse all other attributes of our data. Instead, we will define a vector of attributes, that seems most relevant for the further analysis:

```{r}
chosen.attributes <- c("BALANCE", "balance.to.limit.ratio", "CASH_ADVANCE", "CASH_ADVANCE_FREQUENCY",
                      "CREDIT_LIMIT", "INSTALLMENTS_PURCHASES", "ONEOFF_PURCHASES", "payment.to.minimum.ratio",
                       "PRC_FULL_PAYMENT", "PURCHASES", "PURCHASES_FREQUENCY", "TENURE", "cluster")
```

With this vector we will cut off 8 attributes from the original dataset.

Now we can look at the scatter plots of some chosen attributes, where the instances of different colors belong to different clusters:

```{r}
ggpairs(cbind(cards[chosen.attributes], Cluster = as.factor(cards$cluster)),
        columns = 1:6, aes(colour = Cluster, alpha = 0.5),
        lower = list(continuous = "points"),
        upper = list(continuous = "blank"),
        axisLabels = "none", switch = "both") +
        scale_fill_manual(values = clusters_palette) +
        scale_color_manual(values = clusters_palette) +
        theme_bw()
```


Next, let's look at the mean values of different attributes for each of the clusters are. Grouping by cluster is performed with the group_by() function. The mean values are calculated with the help of the summarise_all() function with the "mean" as input:

```{r}
cards[chosen.attributes] %>%
  group_by(cluster) %>%
  summarise_all(mean)
```

With the help of the geom_histogram(), we can build histogramms for each attribute of every cluster. It will give us a representation of how each variable is distributed:

```{r}

for (i in 1:pca_cards_clust$iter)
{
  print(cards[cards$cluster == i, chosen.attributes] %>%
          gather(Attributes, value, -"cluster",) %>%
          ggplot(aes(x = value, fill = Attributes)) +
          geom_histogram(colour = "white", show.legend = FALSE, bins = 40) +
          facet_wrap(~Attributes, scales = "free", ncol = 3) +
          labs(x = "Values", y = "Frequency",
               title = paste("Cards Attributes - Histograms, Cluster:", i, sep = " ")) +
          theme_grey())
}
```

We can see the following behaviour of the clusters:

* **Cluster 1**. These are the customers, who do not pay much cash in advance (most of them seem not to spend anything at all this way); Spend a bit more on more purchases; Spend approximately same amount on one-off and installment purchases (even though the most do not even do any); Most make either no purchases or make them very often; Have little balance (in compassion to others) - for the majority it is equal or close to 0 (that follows the fact, that many people of this group do not make purchases); They have the smallest credit limit; As very little is spent on purchases, very few people of this group use the full limit of the card (although there are still some); The average payment to minimum ratio makes us believe, that most of the group pay their minimum due; Has more people who have been using their credit card since recently, than other clusters.

* **Cluster 2**. These customers have bigger balance than the first cluster; Much more people have something on the balance at all; As the first group, they do not pay cash in advance frequently and seem much more interested in making purchases - they all do them very often; They spent on cash in advance a lot smaller amount of money as on purchases; For one-off purchases is paid more, than for installment purchases; Even though it is the smallest group, they spend on purchases more than any other group; Although they spend a lot, they pay what they due relatively on time and have the highest percentage of full payments; Have a relatively high credit limit, but seem to copy with it quite well.

* **Cluster 3**. Payment-to-minimum ratio is the lowest. Balance-to-limit ratio is high and for many goes over 100% - it could partially consist of the balances of the earlier months, that has not been paid for (that's why it could be more than the limit); They have a high credit limit and pay often and a lot cash-in-advance; They pay cash-in-advance more than any other group; They make fewer purchases, than anybody else; Not even 5% of them pay all the payments they due; Some have a really high credit limit.

Now, we will plot the histograms of different clusters together. It will help us compare the clusters with each other:

```{r}
cards[chosen.attributes] %>%
  gather(Attributes, value, -"cluster",) %>%
  ggplot(aes(x = value, fill = as.factor(cluster))) +
  scale_fill_manual(values = clusters_palette) +
  geom_histogram(colour = "white", show.legend = FALSE, bins = 40) +
  facet_wrap(~Attributes, scales = "free", ncol = 3) +
  labs(x = "Values", y = "Frequency",
       title = "Cards Attributes - Histograms. Clusters in Comparison") +
  theme_grey()
```

It would also be quite interesting to see, how many clients do not pay the minimum, that they should pay to the bank in order to avoid the interest:

```{r}
cards %>%
   group_by(cluster) %>%
   summarize(paymentLessThanMinimum  = sum(payment.to.minimum.ratio < 1),
              instancesOfCluster = length(cluster)) %>%
   mutate(customersNotPayingMinimum = paymentLessThanMinimum/instancesOfCluster)
```

As a result - quite a big percentage of people to do not make minimum payments. The situation is however better for the 2nd cluster. In the third cluster, over 40% of clients do not make minimum payments - the client from this cluster might have the worst credit score.

The other way to represent the relations among the variables is to build a parallel coordinate plot. We will therefore define a function show_paral_coordnates() to so so:


```{r}
show_paral_coordnates <- function(dataset, clusters_info, cluster_to_sort_by) {
  datatset_with_clusters_info <- dataset
  datatset_with_clusters_info$cluster <- as.factor(clusters_info$cluster)
  dataset_avg <- (datatset_with_clusters_info %>%
    group_by(cluster) %>%
    summarize_if(is.numeric, mean, na.rm = TRUE))
  dataset_avg_sorted <- dataset_avg[2:length(dataset_avg)][, order(dataset_avg[cluster_to_sort_by, 2:length(dataset_avg)])]
  dataset_avg_sorted$cluster <- dataset_avg$cluster
  ggparcoord(dataset_avg_sorted, columns = 1:(length(dataset_avg_sorted) - 1),
             groupColumn = "cluster", scale = "globalminmax") +
    scale_color_manual(values = clusters_palette) +
    theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1))
}
```
We will use the normalized variables to be able to represent them all on the same plot.
Building a parallel coordinate plot:
```{r}
cards_norm$cluster <- pca_cards_clust$cluster
cards_norm$balance.to.limit.ratio <- cards_norm$BALANCE / cards_norm$CREDIT_LIMIT
cards_norm$payment.to.minimum.ratio <- cards_norm$PAYMENTS / cards_norm$MINIMUM_PAYMENTS
show_paral_coordnates(cards_norm[chosen.attributes], pca_cards_clust, 2)
```

Here we can see, how the value of balance-to-limit is extremely high for the 3rd cluster in comparison to the other two. At the same time, they make at fewest full payments.

Now we can draw a conclusion, what the customers in each of the clusters are like:

* **Cluster 1**. Hardly use the benefits of their credit cards. Some of them are new customers, although a majority has probably not been using it much for months. They do not seem to be an interesting target for the bank, though it could be worth a try to work on advertisements of the bank services for this group of people.

* **Cluster 2**. Use their credit card a lot. These clients are loyal, make a lot of purchases and have good credit history. The bank must have a lot of benefit from them.

* **Cluster 3**. These customers do not make many purchases - probably because their limit is already full, and even the balance is being transferred from earlier months. They do not really pay on time what they due to the bank. These customers seem not to be able to handle a high credit limit. The bank must get much interest from these clients as they do not pay enough minimum payments.


## Conclusion

In conclusion, K-means is a powerful and widely used clustering algorithm that can be used to group similar data points together. We have learned, that unsupervised machine learning algorithms are applicable to the task of credit cards customers segmentation. Clustering can help identify patterns and similarities among credit card holders, such as common spending habits and payment patterns.

The optimal number of clusters can be determined by using techniques such as the elbow method and silhouette method, both of which were demonstrated in this notebook.

To reduce dimension of the data, Principal Component Analysis can be used.

Additionally, it is important to keep in mind, that the results of K-means are sensitive to the initial conditions of the algorithm and the choice of initial centroids. To overcome this, it is recommended to run the algorithm multiple times with different initial centroids and select the best solution.

Furthermore, it is important to interpret the results with domain knowledge. We have gotten 3 clusters, representing 3 types of credit card holders: those, who rarely use the benefits of their credit cards, the most loyal customers, who make most purchases and full payments and a third group of people, who do not pay in time and must get interest on what they owe to the bank.

Clustering can help the banks personalize their products and services in order to target certain groups of customers.

This notebook provided an introduction to K-means and demonstrated its implementation in R.